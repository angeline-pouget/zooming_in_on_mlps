{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import timm\n",
    "import detectors\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn \n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import BytesField, IntField, RGBImageField\n",
    "\n",
    "from data_utils.data_stats import *\n",
    "from models.networks import get_model\n",
    "from data_utils.dataloader import get_loader\n",
    "from data_utils.dataset_to_beton import get_dataset\n",
    "from data_utils import texture_shape_transformations as ts\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_model(dataset, model, dataset_type, data_path='C:\\\\Users\\\\Leonidas\\\\OneDrive\\\\Desktop\\\\zooming_in_on_mlps\\\\beton', split='test', batch_size=100):\n",
    "    \"\"\"\n",
    "    This function retrieves the data, model and feature extractor (if needed) based on the provided information.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (str): The name of the dataset to retrieve (can be cifar10, cifar100 or imagenet).\n",
    "    model (str): The name of the model to retrieve (can be mlp, cnn or vit; only mlp is supported for dataset imagenet).\n",
    "    data_path (str): The path to the data.\n",
    "\n",
    "    Returns (as a tuple):\n",
    "    data_loader (DataLoader): The retrieved data loader.\n",
    "    model (Model): The retrieved model.\n",
    "\n",
    "    Raises:\n",
    "    AssertionError: If the dataset or model is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    assert dataset in ('cifar10', 'cifar100', 'imagenet'), f'dataset {dataset} is currently not supported by this function'\n",
    "    assert model in ('mlp', 'cnn', 'vit'), f'model {model} is currently not supported by this function'\n",
    "\n",
    "    num_classes = CLASS_DICT[dataset]\n",
    "    eval_batch_size = batch_size\n",
    " \n",
    "    if dataset == 'imagenet':\n",
    "        data_resolution = 64\n",
    "        assert model == 'mlp', f'imagenet dataset is only supported by mlp model'\n",
    "    else:\n",
    "        data_resolution = 32\n",
    "\n",
    "    crop_resolution = data_resolution\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device == 'cuda':\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    if model == 'mlp':\n",
    "        architecture = 'B_12-Wi_1024'\n",
    "        checkpoint = 'in21k_' + dataset\n",
    "        model = get_model(architecture=architecture, resolution=64, num_classes=num_classes, checkpoint=checkpoint)\n",
    "\n",
    "    if model == 'cnn':\n",
    "        architecture = 'resnet18_' + dataset\n",
    "        model = timm.create_model(architecture, pretrained=True)\n",
    "\n",
    "    if model == 'vit':\n",
    "        architecture = 'vit_small_patch16_224_' + dataset + '_v7.pth'\n",
    "        model = torch.load(architecture)\n",
    "        \n",
    "    if (dataset_type=='stylized') == False : \n",
    "        data_loader = ts.get_Shapeloader(\n",
    "            dataset   = dataset,\n",
    "            bs        = eval_batch_size,\n",
    "            mode      = split,\n",
    "            augment=split == 'train',\n",
    "            dev       = device,\n",
    "            mixup     = 0.0,\n",
    "            data_path = data_path,\n",
    "            data_resolution = data_resolution,\n",
    "            crop_resolution = crop_resolution,\n",
    "            dataset_type    = dataset_type\n",
    "            )\n",
    "    \n",
    "    if dataset_type == 'stylized':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((crop_resolution, crop_resolution)),  \n",
    "            transforms.ToTensor(),])\n",
    "\n",
    "        stylized_dataset = ts.StylizedDataset('C:\\\\Users\\\\Leonidas\\\\OneDrive\\\\Desktop\\\\zooming_in_on_mlps\\\\stylized_datasets\\\\cifar10_None', transform=transform)\n",
    "\n",
    "        data_loader = ts.DataLoader(stylized_dataset, batch_size=eval_batch_size, shuffle=True)\n",
    "        \n",
    "\n",
    "    return data_loader, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module): \n",
    "    def __init__(self, shape=224): \n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape \n",
    "        \n",
    "    def forward(self, x): \n",
    "        shape = self.shape\n",
    "        x = transforms.functional.resize(x, size=(shape, shape))\n",
    "        if shape == 64:\n",
    "            bs = x.shape[0]\n",
    "            x = torch.reshape(x, shape=(bs,-1,))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar10'\n",
    "model_name = 'mlp'\n",
    "dataset_type = 'grayscale'\n",
    "\n",
    "loader, model = get_data_and_model(dataset=dataset_name, model=model_name, dataset_type=dataset_type, data_path='C:\\\\Users\\\\Leonidas\\\\OneDrive\\\\Desktop\\\\zooming_in_on_mlps\\\\beton', split='test', batch_size=512)\n",
    "\n",
    "if model_name == 'mlp':\n",
    "    model = nn.Sequential(Reshape(64), model)\n",
    "\n",
    "if model_name == 'vit':\n",
    "    model = nn.Sequential(Reshape(224), model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image viewing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter    = iter(loader)\n",
    "    ims, labels = next(dataiter)\n",
    "    img_to_show = ims[225]\n",
    "    print(ims.shape, labels.shape)\n",
    "    \n",
    "    if not isinstance(img_to_show, np.ndarray):\n",
    "        img_to_show = img_to_show.numpy()\n",
    "    \n",
    "    print(img_to_show.shape)\n",
    "    # for shuffled images need to add .astype(np.uint8)\n",
    "    plt.imshow(img_to_show.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a test function that evaluates the test accuracy of the loaded model/transformation \n",
    "# for every class of cifar10 .\n",
    "\n",
    "@torch.no_grad()    \n",
    "def test_class_n(model, loader,n):\n",
    "    model.eval()\n",
    "    class_n_acc = AverageMeter()\n",
    "    \n",
    "    \n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        \n",
    "        preds = model(ims)\n",
    "        \n",
    "        targs_new_list = []\n",
    "        preds_new_list = []\n",
    "        \n",
    "        for i in range(len(targs)) :\n",
    "            if targs[i] == n:\n",
    "                targs_new_list.append(targs[i])\n",
    "                preds_new_list.append(preds[i])\n",
    "                \n",
    "        if not targs_new_list:\n",
    "            continue \n",
    "                \n",
    "        targs_new = torch.stack(targs_new_list)\n",
    "        preds_new = torch.stack(preds_new_list)        \n",
    "        \n",
    "        acc = real_acc(preds_new, targs_new, k=5, avg=True)\n",
    "        class_n_acc.update(acc, ims.shape[0]) \n",
    "        \n",
    "           \n",
    "    return (\n",
    "        class_n_acc.get_avg(percentage=True),\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_1 = test_class_n(model, loader, n=0)\n",
    "tuple_2 = test_class_n(model, loader, n=1)\n",
    "tuple_3 = test_class_n(model, loader, n=2)\n",
    "tuple_4 = test_class_n(model, loader, n=3)\n",
    "tuple_5 = test_class_n(model, loader, n=4)\n",
    "tuple_6 = test_class_n(model, loader, n=5)\n",
    "tuple_7 = test_class_n(model, loader, n=6)\n",
    "tuple_8 = test_class_n(model, loader, n=7)\n",
    "tuple_9 = test_class_n(model, loader, n=8)\n",
    "tuple_10 = test_class_n(model, loader, n=9)\n",
    "\n",
    "\n",
    "test_class_1_acc = np.array(tuple_1[0].item()) \n",
    "test_class_2_acc = np.array(tuple_2[0].item())\n",
    "test_class_3_acc = np.array(tuple_3[0].item())\n",
    "test_class_4_acc = np.array(tuple_4[0].item())\n",
    "test_class_5_acc = np.array(tuple_5[0].item())\n",
    "test_class_6_acc = np.array(tuple_6[0].item()) \n",
    "test_class_7_acc = np.array(tuple_7[0].item())\n",
    "test_class_8_acc = np.array(tuple_8[0].item())\n",
    "test_class_9_acc = np.array(tuple_9[0].item())\n",
    "test_class_10_acc = np.array(tuple_10[0].item())\n",
    "\n",
    "test_all_classes_acc = np.array([test_class_1_acc, test_class_2_acc, test_class_3_acc, test_class_4_acc, test_class_5_acc, test_class_6_acc, test_class_7_acc, test_class_8_acc, test_class_9_acc, test_class_10_acc])\n",
    "\n",
    "print(test_all_classes_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "acc_B_12_Wi_1024_grayscale = np.array( [90.60163116, 86.58802032, 71.03863525, 82.86347198, 85.42298126, 59.4724617,\n",
    " 78.10970306, 87.77689362, 89.53246307, 86.39989471] )\n",
    "\n",
    "acc_Vit_grayscale = np.array([88.71141815, 78.76961517, 78.72644043, 82.62313843, 56.64732361, 63.65930176,\n",
    " 86.56697845, 88.33841705, 90.11930847, 92.89593506])\n",
    "\n",
    "acc_CNN_grayscale = np.array([90.61014557, 93.86965942, 80.90284729, 90.144104,   82.69236755, 81.28423309,\n",
    " 88.97644806, 89.74946594, 94.26036072, 93.65608978])\n",
    "\n",
    "acc_B_12_Wi_1024_occluded = np.array([62.62025452, 18.02142715, 39.26353836, 66.94885254, 54.81447983, 38.47133255,\n",
    " 34.01692963, 43.2136879,  46.14099503, 86.99600983])\n",
    "\n",
    "acc_Vit_occluded = np.array([73.3727951,  46.33624268, 68.18949127, 72.34455872, 71.52188873, 56.89443588,\n",
    " 65.12810516, 74.4233551,  72.77259064, 90.13330078])\n",
    "\n",
    "acc_CNN_occluded = np.array([55.28726959, 37.82196426, 27.41372108, 37.90256119, 19.68580627, 21.96700096,\n",
    "  9.89237499, 25.03257751, 54.73659515, 78.98101044])\n",
    "\n",
    "acc_B_12_Wi_1024_shuffled = np.array([42.78529358,  8.43040085, 27.42309761, 61.9797821,  39.41212845, 19.15467072,\n",
    " 24.63440704,  9.54283428, 13.25744343, 64.33995056])\n",
    "\n",
    "acc_Vit_shuffled = np.array([61.06734848, 39.21717453, 58.58289337, 76.94311523, 36.9121933,  16.78746414,\n",
    " 53.30687332, 50.89263916, 37.50557327, 84.60777283])\n",
    "\n",
    "acc_CNN_shuffled = np.array([49.53892136,  6.10336494, 16.64393044, 31.72009468, 34.00931168,  7.60824776,\n",
    " 14.88562965, 17.25509071, 43.15248489, 87.3731308 ])\n",
    "\n",
    "acc_B_12_Wi_1024_edged = np.array([33.13269424,  1.76605415, 20.59712219, 57.07995605,  9.82701015,  3.84182692,\n",
    " 57.98770142, 27.85633087, 18.47223473, 23.39993477])\n",
    "\n",
    "acc_Vit_edged = np.array([15.29953861,  2.16743994, 76.87985992, 36.87343597,  5.37955189,  6.39922523,\n",
    "  5.25006056, 10.56084728, 69.62446594,  4.98120117])\n",
    "\n",
    "acc_CNN_edged = np.array([51.38577652, 37.54698944, 27.07089043, 87.02065277, 34.97031403, 32.18006897,\n",
    " 25.10679817, 46.36006165, 46.44708252, 22.46714211])\n",
    "\n",
    "acc_B_12_Wi_1024_stylized = np.array([42.35424805,  4.62544441, 33.88447952, 81.73014069, 44.40874863, 18.18210411,\n",
    " 18.58835411, 35.54258347, 45.92625809, 45.1654892 ])\n",
    "\n",
    "acc_CNN_stylized = np.array([ 9.97588062,  0.22400002,  2.05261755, 77.90389252,  2.09117484, 41.80592346,\n",
    "  5.49640369,  6.81733847, 38.77479553,  0.87649888])\n",
    "\n",
    "acc_Vit_stylized = np.array([40.43080902,  4.56661367, 29.20985031, 53.48271942, 58.1090126,  32.15496063,\n",
    " 27.77684593, 28.61413765, 65.32761383, 28.3950367 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the class accuracies for the 3 models tested on modified images of cifar10 \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc_model1 = acc_B_12_Wi_1024_stylized\n",
    "acc_model2 = acc_CNN_stylized\n",
    "acc_model3 = acc_Vit_stylized\n",
    "class_labels = ['AIRPLANE', 'AUTOMOBILE', 'BIRD', 'CAT', 'DEER', 'DOG', 'FROG', 'HORSE', 'SHIP', 'TRUCK']\n",
    "\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(class_labels))\n",
    "\n",
    "plt.barh(index, acc_model1, height=bar_width, label='B_12-Wi_1024')\n",
    "plt.barh(index + bar_width, acc_model2, height=bar_width, label='Resnet_18-CNN')\n",
    "plt.barh(index + 2 * bar_width, acc_model3, height=bar_width, label='ViT')\n",
    "\n",
    "plt.yticks(index + bar_width, class_labels)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('CIFAR10 Class Labels')\n",
    "plt.title('Accuracies on Stylized Images')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
