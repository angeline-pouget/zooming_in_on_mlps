{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration of MLP, CNN and ViT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model_utils import get_test_data_and_model\n",
    "from utils.metrics import topk_acc, AverageMeter\n",
    "from utils.reliability_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distribution of confidence for correctly classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function that evaluates test accuracy\n",
    "@torch.no_grad()\n",
    "def test(model, loader, model_name=None):\n",
    "    total_acc, total_top5 = AverageMeter(), AverageMeter()\n",
    "    all_conf, true_label_conv, true_label_corr_conf = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        preds = model(ims)\n",
    "        acc, top5 = topk_acc(preds, targs, k=5, avg=True)\n",
    "        total_acc.update(acc, ims.shape[0])\n",
    "        total_top5.update(top5, ims.shape[0])\n",
    "\n",
    "        p = torch.nn.functional.softmax(preds, dim = 1)\n",
    "        true_label_p = p[range(ims.shape[0]), targs]\n",
    "        pred_labels = preds.argmax(dim=1)\n",
    "        true_label_corr_p = true_label_p[pred_labels == targs]\n",
    "\n",
    "        all_conf.append(p.flatten().cpu().numpy())\n",
    "        true_label_conv.append(true_label_p.flatten().cpu().numpy())\n",
    "        true_label_corr_conf.append(true_label_corr_p.flatten().cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        total_acc.get_avg(percentage=True),\n",
    "        total_top5.get_avg(percentage=True),\n",
    "        all_conf,\n",
    "        true_label_conv,\n",
    "        true_label_corr_conf,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar10'\n",
    "model_name = 'mlp'\n",
    "\n",
    "data_loader, model = get_test_data_and_model(dataset=dataset_name, model=model_name, data_path='/scratch/data/ffcv/')\n",
    "test_acc, test_top5, all_conf, true_label_conv, true_label_corr_conf = test(model, data_loader, model_name)\n",
    "\n",
    "# Print all the stats\n",
    "print(\"Test Accuracy        \", \"{:.4f}\".format(test_acc))\n",
    "print(\"Top 5 Test Accuracy          \", \"{:.4f}\".format(test_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "plt.hist(flatten_comprehension(true_label_corr_conf), bins=100)\n",
    "plt.title('Correct Class Confidences for MLP', fontsize=18)\n",
    "plt.xlabel('Confidence', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights already downloaded\n",
      "Load_state output <All keys matched successfully>\n",
      "Loading /scratch/data/ffcv/cifar10/val_32.beton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/10 [00:00<?, ?it/s]/home/apouget/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Evaluation:  10%|â–ˆ         | 1/10 [00:12<01:54, 12.70s/it]"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cifar10'\n",
    "model_name = 'mlp'\n",
    "\n",
    "dataloader, model = get_test_data_and_model(dataset=dataset_name, model=model_name, data_path='/scratch/data/ffcv/')\n",
    "\n",
    "def reliability_diagram(model, dataloader):\n",
    "    model.eval()\n",
    "    conf = torch.tensor([])\n",
    "    pred = torch.tensor([])\n",
    "    targ = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ims, targs in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            preds = model(ims)\n",
    "            p = torch.nn.functional.softmax(preds, dim = 1)\n",
    "            confidence_vals, predictions = torch.max(p, dim=1)\n",
    "            conf = torch.cat((conf, confidence_vals))\n",
    "            pred = torch.cat((pred, predictions))\n",
    "            targ = torch.cat((targ, targs))\n",
    "            \n",
    "    print(\"ECE: \", expected_calibration_error(conf, pred, targ))\n",
    "    reliability_plot(conf, pred, targ)\n",
    "\n",
    "reliability_diagram(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
