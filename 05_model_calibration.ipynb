{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration of MLP, CNN and ViT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model_utils import get_test_data_and_model\n",
    "from utils.metrics import topk_acc, AverageMeter\n",
    "from utils.reliability_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distribution of confidence for correctly classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function that evaluates test accuracy\n",
    "@torch.no_grad()\n",
    "def test(model, loader, model_name=None):\n",
    "    total_acc, total_top5 = AverageMeter(), AverageMeter()\n",
    "    all_conf, true_label_conv, true_label_corr_conf = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        preds = model(ims)\n",
    "        acc, top5 = topk_acc(preds, targs, k=5, avg=True)\n",
    "        total_acc.update(acc, ims.shape[0])\n",
    "        total_top5.update(top5, ims.shape[0])\n",
    "\n",
    "        p = torch.nn.functional.softmax(preds, dim = 1)\n",
    "        true_label_p = p[range(ims.shape[0]), targs]\n",
    "        pred_labels = preds.argmax(dim=1)\n",
    "        true_label_corr_p = true_label_p[pred_labels == targs]\n",
    "\n",
    "        all_conf.append(p.flatten().cpu().numpy())\n",
    "        true_label_conv.append(true_label_p.flatten().cpu().numpy())\n",
    "        true_label_corr_conf.append(true_label_corr_p.flatten().cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        total_acc.get_avg(percentage=True),\n",
    "        total_top5.get_avg(percentage=True),\n",
    "        all_conf,\n",
    "        true_label_conv,\n",
    "        true_label_corr_conf,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar10'\n",
    "model_name = 'mlp'\n",
    "\n",
    "data_loader, model = get_test_data_and_model(dataset=dataset_name, model=model_name, data_path='/scratch/data/ffcv/')\n",
    "test_acc, test_top5, all_conf, true_label_conv, true_label_corr_conf = test(model, data_loader, model_name)\n",
    "\n",
    "# Print all the stats\n",
    "print(\"Test Accuracy        \", \"{:.4f}\".format(test_acc))\n",
    "print(\"Top 5 Test Accuracy          \", \"{:.4f}\".format(test_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "plt.hist(flatten_comprehension(true_label_corr_conf), bins=100)\n",
    "plt.title('Correct Class Confidences for MLP', fontsize=18)\n",
    "plt.xlabel('Confidence', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram(model, dataloader):\n",
    "    model.eval()\n",
    "    conf = torch.tensor([])\n",
    "    pred = torch.tensor([])\n",
    "    targ = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ims, targs in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            preds = model(ims)\n",
    "            p = torch.nn.functional.softmax(preds, dim = 1)\n",
    "            confidence_vals, predictions = torch.max(p, dim=1)\n",
    "            conf = torch.cat((conf, confidence_vals))\n",
    "            pred = torch.cat((pred, predictions))\n",
    "            targ = torch.cat((targ, targs))\n",
    "            \n",
    "    print(\"ECE: \", expected_calibration_error(conf, pred, targ))\n",
    "    reliability_plot(conf, pred, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar10'\n",
    "model_name = 'mlp'\n",
    "\n",
    "dataloader, model = get_test_data_and_model(dataset=dataset_name, model=model_name, data_path='/scratch/data/ffcv/')\n",
    "reliability_diagram(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
