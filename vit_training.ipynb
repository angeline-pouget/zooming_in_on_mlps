{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning ViT on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from ffcv.fields import BytesField, IntField, RGBImageField\n",
    "from ffcv.writer import DatasetWriter\n",
    "\n",
    "from data_utils.data_stats import *\n",
    "from data_utils.dataloader import get_loader\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter\n",
    "from models.networks import get_model\n",
    "from data_utils.dataset_to_beton import get_dataset\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 384, 14, 14]         295,296\n",
      "          Identity-2             [-1, 196, 384]               0\n",
      "        PatchEmbed-3             [-1, 196, 384]               0\n",
      "           Dropout-4             [-1, 197, 384]               0\n",
      "          Identity-5             [-1, 197, 384]               0\n",
      "         LayerNorm-6             [-1, 197, 384]             768\n",
      "            Linear-7            [-1, 197, 1152]         443,520\n",
      "          Identity-8           [-1, 6, 197, 64]               0\n",
      "          Identity-9           [-1, 6, 197, 64]               0\n",
      "           Linear-10             [-1, 197, 384]         147,840\n",
      "          Dropout-11             [-1, 197, 384]               0\n",
      "        Attention-12             [-1, 197, 384]               0\n",
      "         Identity-13             [-1, 197, 384]               0\n",
      "         Identity-14             [-1, 197, 384]               0\n",
      "        LayerNorm-15             [-1, 197, 384]             768\n",
      "           Linear-16            [-1, 197, 1536]         591,360\n",
      "             GELU-17            [-1, 197, 1536]               0\n",
      "          Dropout-18            [-1, 197, 1536]               0\n",
      "           Linear-19             [-1, 197, 384]         590,208\n",
      "          Dropout-20             [-1, 197, 384]               0\n",
      "              Mlp-21             [-1, 197, 384]               0\n",
      "         Identity-22             [-1, 197, 384]               0\n",
      "         Identity-23             [-1, 197, 384]               0\n",
      "            Block-24             [-1, 197, 384]               0\n",
      "        LayerNorm-25             [-1, 197, 384]             768\n",
      "           Linear-26            [-1, 197, 1152]         443,520\n",
      "         Identity-27           [-1, 6, 197, 64]               0\n",
      "         Identity-28           [-1, 6, 197, 64]               0\n",
      "           Linear-29             [-1, 197, 384]         147,840\n",
      "          Dropout-30             [-1, 197, 384]               0\n",
      "        Attention-31             [-1, 197, 384]               0\n",
      "         Identity-32             [-1, 197, 384]               0\n",
      "         Identity-33             [-1, 197, 384]               0\n",
      "        LayerNorm-34             [-1, 197, 384]             768\n",
      "           Linear-35            [-1, 197, 1536]         591,360\n",
      "             GELU-36            [-1, 197, 1536]               0\n",
      "          Dropout-37            [-1, 197, 1536]               0\n",
      "           Linear-38             [-1, 197, 384]         590,208\n",
      "          Dropout-39             [-1, 197, 384]               0\n",
      "              Mlp-40             [-1, 197, 384]               0\n",
      "         Identity-41             [-1, 197, 384]               0\n",
      "         Identity-42             [-1, 197, 384]               0\n",
      "            Block-43             [-1, 197, 384]               0\n",
      "        LayerNorm-44             [-1, 197, 384]             768\n",
      "           Linear-45            [-1, 197, 1152]         443,520\n",
      "         Identity-46           [-1, 6, 197, 64]               0\n",
      "         Identity-47           [-1, 6, 197, 64]               0\n",
      "           Linear-48             [-1, 197, 384]         147,840\n",
      "          Dropout-49             [-1, 197, 384]               0\n",
      "        Attention-50             [-1, 197, 384]               0\n",
      "         Identity-51             [-1, 197, 384]               0\n",
      "         Identity-52             [-1, 197, 384]               0\n",
      "        LayerNorm-53             [-1, 197, 384]             768\n",
      "           Linear-54            [-1, 197, 1536]         591,360\n",
      "             GELU-55            [-1, 197, 1536]               0\n",
      "          Dropout-56            [-1, 197, 1536]               0\n",
      "           Linear-57             [-1, 197, 384]         590,208\n",
      "          Dropout-58             [-1, 197, 384]               0\n",
      "              Mlp-59             [-1, 197, 384]               0\n",
      "         Identity-60             [-1, 197, 384]               0\n",
      "         Identity-61             [-1, 197, 384]               0\n",
      "            Block-62             [-1, 197, 384]               0\n",
      "        LayerNorm-63             [-1, 197, 384]             768\n",
      "           Linear-64            [-1, 197, 1152]         443,520\n",
      "         Identity-65           [-1, 6, 197, 64]               0\n",
      "         Identity-66           [-1, 6, 197, 64]               0\n",
      "           Linear-67             [-1, 197, 384]         147,840\n",
      "          Dropout-68             [-1, 197, 384]               0\n",
      "        Attention-69             [-1, 197, 384]               0\n",
      "         Identity-70             [-1, 197, 384]               0\n",
      "         Identity-71             [-1, 197, 384]               0\n",
      "        LayerNorm-72             [-1, 197, 384]             768\n",
      "           Linear-73            [-1, 197, 1536]         591,360\n",
      "             GELU-74            [-1, 197, 1536]               0\n",
      "          Dropout-75            [-1, 197, 1536]               0\n",
      "           Linear-76             [-1, 197, 384]         590,208\n",
      "          Dropout-77             [-1, 197, 384]               0\n",
      "              Mlp-78             [-1, 197, 384]               0\n",
      "         Identity-79             [-1, 197, 384]               0\n",
      "         Identity-80             [-1, 197, 384]               0\n",
      "            Block-81             [-1, 197, 384]               0\n",
      "        LayerNorm-82             [-1, 197, 384]             768\n",
      "           Linear-83            [-1, 197, 1152]         443,520\n",
      "         Identity-84           [-1, 6, 197, 64]               0\n",
      "         Identity-85           [-1, 6, 197, 64]               0\n",
      "           Linear-86             [-1, 197, 384]         147,840\n",
      "          Dropout-87             [-1, 197, 384]               0\n",
      "        Attention-88             [-1, 197, 384]               0\n",
      "         Identity-89             [-1, 197, 384]               0\n",
      "         Identity-90             [-1, 197, 384]               0\n",
      "        LayerNorm-91             [-1, 197, 384]             768\n",
      "           Linear-92            [-1, 197, 1536]         591,360\n",
      "             GELU-93            [-1, 197, 1536]               0\n",
      "          Dropout-94            [-1, 197, 1536]               0\n",
      "           Linear-95             [-1, 197, 384]         590,208\n",
      "          Dropout-96             [-1, 197, 384]               0\n",
      "              Mlp-97             [-1, 197, 384]               0\n",
      "         Identity-98             [-1, 197, 384]               0\n",
      "         Identity-99             [-1, 197, 384]               0\n",
      "           Block-100             [-1, 197, 384]               0\n",
      "       LayerNorm-101             [-1, 197, 384]             768\n",
      "          Linear-102            [-1, 197, 1152]         443,520\n",
      "        Identity-103           [-1, 6, 197, 64]               0\n",
      "        Identity-104           [-1, 6, 197, 64]               0\n",
      "          Linear-105             [-1, 197, 384]         147,840\n",
      "         Dropout-106             [-1, 197, 384]               0\n",
      "       Attention-107             [-1, 197, 384]               0\n",
      "        Identity-108             [-1, 197, 384]               0\n",
      "        Identity-109             [-1, 197, 384]               0\n",
      "       LayerNorm-110             [-1, 197, 384]             768\n",
      "          Linear-111            [-1, 197, 1536]         591,360\n",
      "            GELU-112            [-1, 197, 1536]               0\n",
      "         Dropout-113            [-1, 197, 1536]               0\n",
      "          Linear-114             [-1, 197, 384]         590,208\n",
      "         Dropout-115             [-1, 197, 384]               0\n",
      "             Mlp-116             [-1, 197, 384]               0\n",
      "        Identity-117             [-1, 197, 384]               0\n",
      "        Identity-118             [-1, 197, 384]               0\n",
      "           Block-119             [-1, 197, 384]               0\n",
      "       LayerNorm-120             [-1, 197, 384]             768\n",
      "          Linear-121            [-1, 197, 1152]         443,520\n",
      "        Identity-122           [-1, 6, 197, 64]               0\n",
      "        Identity-123           [-1, 6, 197, 64]               0\n",
      "          Linear-124             [-1, 197, 384]         147,840\n",
      "         Dropout-125             [-1, 197, 384]               0\n",
      "       Attention-126             [-1, 197, 384]               0\n",
      "        Identity-127             [-1, 197, 384]               0\n",
      "        Identity-128             [-1, 197, 384]               0\n",
      "       LayerNorm-129             [-1, 197, 384]             768\n",
      "          Linear-130            [-1, 197, 1536]         591,360\n",
      "            GELU-131            [-1, 197, 1536]               0\n",
      "         Dropout-132            [-1, 197, 1536]               0\n",
      "          Linear-133             [-1, 197, 384]         590,208\n",
      "         Dropout-134             [-1, 197, 384]               0\n",
      "             Mlp-135             [-1, 197, 384]               0\n",
      "        Identity-136             [-1, 197, 384]               0\n",
      "        Identity-137             [-1, 197, 384]               0\n",
      "           Block-138             [-1, 197, 384]               0\n",
      "       LayerNorm-139             [-1, 197, 384]             768\n",
      "          Linear-140            [-1, 197, 1152]         443,520\n",
      "        Identity-141           [-1, 6, 197, 64]               0\n",
      "        Identity-142           [-1, 6, 197, 64]               0\n",
      "          Linear-143             [-1, 197, 384]         147,840\n",
      "         Dropout-144             [-1, 197, 384]               0\n",
      "       Attention-145             [-1, 197, 384]               0\n",
      "        Identity-146             [-1, 197, 384]               0\n",
      "        Identity-147             [-1, 197, 384]               0\n",
      "       LayerNorm-148             [-1, 197, 384]             768\n",
      "          Linear-149            [-1, 197, 1536]         591,360\n",
      "            GELU-150            [-1, 197, 1536]               0\n",
      "         Dropout-151            [-1, 197, 1536]               0\n",
      "          Linear-152             [-1, 197, 384]         590,208\n",
      "         Dropout-153             [-1, 197, 384]               0\n",
      "             Mlp-154             [-1, 197, 384]               0\n",
      "        Identity-155             [-1, 197, 384]               0\n",
      "        Identity-156             [-1, 197, 384]               0\n",
      "           Block-157             [-1, 197, 384]               0\n",
      "       LayerNorm-158             [-1, 197, 384]             768\n",
      "          Linear-159            [-1, 197, 1152]         443,520\n",
      "        Identity-160           [-1, 6, 197, 64]               0\n",
      "        Identity-161           [-1, 6, 197, 64]               0\n",
      "          Linear-162             [-1, 197, 384]         147,840\n",
      "         Dropout-163             [-1, 197, 384]               0\n",
      "       Attention-164             [-1, 197, 384]               0\n",
      "        Identity-165             [-1, 197, 384]               0\n",
      "        Identity-166             [-1, 197, 384]               0\n",
      "       LayerNorm-167             [-1, 197, 384]             768\n",
      "          Linear-168            [-1, 197, 1536]         591,360\n",
      "            GELU-169            [-1, 197, 1536]               0\n",
      "         Dropout-170            [-1, 197, 1536]               0\n",
      "          Linear-171             [-1, 197, 384]         590,208\n",
      "         Dropout-172             [-1, 197, 384]               0\n",
      "             Mlp-173             [-1, 197, 384]               0\n",
      "        Identity-174             [-1, 197, 384]               0\n",
      "        Identity-175             [-1, 197, 384]               0\n",
      "           Block-176             [-1, 197, 384]               0\n",
      "       LayerNorm-177             [-1, 197, 384]             768\n",
      "          Linear-178            [-1, 197, 1152]         443,520\n",
      "        Identity-179           [-1, 6, 197, 64]               0\n",
      "        Identity-180           [-1, 6, 197, 64]               0\n",
      "          Linear-181             [-1, 197, 384]         147,840\n",
      "         Dropout-182             [-1, 197, 384]               0\n",
      "       Attention-183             [-1, 197, 384]               0\n",
      "        Identity-184             [-1, 197, 384]               0\n",
      "        Identity-185             [-1, 197, 384]               0\n",
      "       LayerNorm-186             [-1, 197, 384]             768\n",
      "          Linear-187            [-1, 197, 1536]         591,360\n",
      "            GELU-188            [-1, 197, 1536]               0\n",
      "         Dropout-189            [-1, 197, 1536]               0\n",
      "          Linear-190             [-1, 197, 384]         590,208\n",
      "         Dropout-191             [-1, 197, 384]               0\n",
      "             Mlp-192             [-1, 197, 384]               0\n",
      "        Identity-193             [-1, 197, 384]               0\n",
      "        Identity-194             [-1, 197, 384]               0\n",
      "           Block-195             [-1, 197, 384]               0\n",
      "       LayerNorm-196             [-1, 197, 384]             768\n",
      "          Linear-197            [-1, 197, 1152]         443,520\n",
      "        Identity-198           [-1, 6, 197, 64]               0\n",
      "        Identity-199           [-1, 6, 197, 64]               0\n",
      "          Linear-200             [-1, 197, 384]         147,840\n",
      "         Dropout-201             [-1, 197, 384]               0\n",
      "       Attention-202             [-1, 197, 384]               0\n",
      "        Identity-203             [-1, 197, 384]               0\n",
      "        Identity-204             [-1, 197, 384]               0\n",
      "       LayerNorm-205             [-1, 197, 384]             768\n",
      "          Linear-206            [-1, 197, 1536]         591,360\n",
      "            GELU-207            [-1, 197, 1536]               0\n",
      "         Dropout-208            [-1, 197, 1536]               0\n",
      "          Linear-209             [-1, 197, 384]         590,208\n",
      "         Dropout-210             [-1, 197, 384]               0\n",
      "             Mlp-211             [-1, 197, 384]               0\n",
      "        Identity-212             [-1, 197, 384]               0\n",
      "        Identity-213             [-1, 197, 384]               0\n",
      "           Block-214             [-1, 197, 384]               0\n",
      "       LayerNorm-215             [-1, 197, 384]             768\n",
      "          Linear-216            [-1, 197, 1152]         443,520\n",
      "        Identity-217           [-1, 6, 197, 64]               0\n",
      "        Identity-218           [-1, 6, 197, 64]               0\n",
      "          Linear-219             [-1, 197, 384]         147,840\n",
      "         Dropout-220             [-1, 197, 384]               0\n",
      "       Attention-221             [-1, 197, 384]               0\n",
      "        Identity-222             [-1, 197, 384]               0\n",
      "        Identity-223             [-1, 197, 384]               0\n",
      "       LayerNorm-224             [-1, 197, 384]             768\n",
      "          Linear-225            [-1, 197, 1536]         591,360\n",
      "            GELU-226            [-1, 197, 1536]               0\n",
      "         Dropout-227            [-1, 197, 1536]               0\n",
      "          Linear-228             [-1, 197, 384]         590,208\n",
      "         Dropout-229             [-1, 197, 384]               0\n",
      "             Mlp-230             [-1, 197, 384]               0\n",
      "        Identity-231             [-1, 197, 384]               0\n",
      "        Identity-232             [-1, 197, 384]               0\n",
      "           Block-233             [-1, 197, 384]               0\n",
      "       LayerNorm-234             [-1, 197, 384]             768\n",
      "        Identity-235                  [-1, 384]               0\n",
      "          Linear-236                  [-1, 100]          38,500\n",
      "================================================================\n",
      "Total params: 21,628,132\n",
      "Trainable params: 38,500\n",
      "Non-trainable params: 21,589,632\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 211.23\n",
      "Params size (MB): 82.50\n",
      "Estimated Total Size (MB): 294.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"vit_small_patch16_224\", pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "outputs_attrs = 100\n",
    "num_inputs = model.head.in_features\n",
    "last_layer = nn.Linear(num_inputs, outputs_attrs)\n",
    "model.head = last_layer\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apouget/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 64, 64)\n",
    "img = T.functional.resize(img, size=(224, 224))\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/ffcv/cifar100/train_32.beton\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loader = get_loader(\n",
    "    \"cifar100\",\n",
    "    bs=128,\n",
    "    mode=\"train\",\n",
    "    augment=True,\n",
    "    dev=device,\n",
    "    mixup=0.0,\n",
    "    data_path='/scratch/ffcv/',\n",
    "    data_resolution=32,\n",
    "    crop_resolution=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ims, targs in tqdm(loader, desc=\"Training\"):\n",
    "    ims = T.functional.resize(ims, size=(224, 224))\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(ims)\n",
    "    loss = loss_function(outputs, targs)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'vit_small_patch16_224_cifar100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function that evaluates test accuracy\n",
    "@torch.no_grad()\n",
    "def test(model, loader, extractor = None):\n",
    "    model.eval()\n",
    "    total_acc, total_top5 = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        ims = T.functional.resize(ims, size=(224, 224))\n",
    "        preds = model(ims)\n",
    "            \n",
    "        acc, top5 = topk_acc(preds, targs, k=5, avg=True)\n",
    "\n",
    "        total_acc.update(acc, ims.shape[0])\n",
    "        total_top5.update(top5, ims.shape[0])\n",
    "\n",
    "    return (\n",
    "        total_acc.get_avg(percentage=True),\n",
    "        total_top5.get_avg(percentage=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/ffcv/cifar10/val_32.beton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/79 [00:00<?, ?it/s]/home/apouget/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Evaluation: 100%|██████████| 79/79 [27:33<00:00, 20.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy         92.2800\n",
      "Top 5 Test Accuracy           99.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(\n",
    "    \"cifar10\",\n",
    "    bs=128,\n",
    "    mode=\"test\",\n",
    "    augment=False,\n",
    "    dev=device,\n",
    "    mixup=0.0,\n",
    "    data_path='/scratch/ffcv/',\n",
    "    data_resolution=32,\n",
    "    crop_resolution=32,\n",
    ")\n",
    "test_acc, test_top5 = test(model, data_loader)\n",
    "\n",
    "# Print all the stats\n",
    "print(\"Test Accuracy        \", \"{:.4f}\".format(test_acc))\n",
    "print(\"Top 5 Test Accuracy          \", \"{:.4f}\".format(test_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/ffcv/cifar100/test_32.beton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/79 [00:00<?, ?it/s]/home/apouget/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Evaluation: 100%|██████████| 79/79 [27:47<00:00, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy         63.1300\n",
      "Top 5 Test Accuracy           85.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(\n",
    "    \"cifar100\",\n",
    "    bs=128,\n",
    "    mode=\"test\",\n",
    "    augment=False,\n",
    "    dev=device,\n",
    "    mixup=0.0,\n",
    "    data_path='/scratch/ffcv/',\n",
    "    data_resolution=32,\n",
    "    crop_resolution=32,\n",
    ")\n",
    "test_acc, test_top5 = test(model, data_loader)\n",
    "\n",
    "# Print all the stats\n",
    "print(\"Test Accuracy        \", \"{:.4f}\".format(test_acc))\n",
    "print(\"Top 5 Test Accuracy          \", \"{:.4f}\".format(test_top5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
