{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Fall 2023 Course Project - Zooming in on MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import detectors\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from ffcv.fields import BytesField, IntField, RGBImageField\n",
    "from ffcv.writer import DatasetWriter\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "from data_utils.data_stats import *\n",
    "from data_utils.dataloader import get_loader\n",
    "from data_utils.dataset_to_beton import get_dataset\n",
    "from models.networks import get_model\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data loader and model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_model(dataset='cifar10', model='mlp', data_path='/scratch/ffcv/'):\n",
    "    \"\"\"\n",
    "    This function retrieves the data, model and feature extractor (if needed) based on the provided information.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (str): The name of the dataset to retrieve (can be cifar10, cifar100 or imagenet).\n",
    "    model (str): The name of the model to retrieve (can be mlp, cnn or vit; only mlp is supported for dataset imagenet).\n",
    "    data_path (str): The path to the data.\n",
    "\n",
    "    Returns:\n",
    "    data_loader (DataLoader): The retrieved data loader.\n",
    "    model (Model): The retrieved model.\n",
    "    feature_extractor (Model): The retrieved feature extractor (only for model vit).\n",
    "\n",
    "    Raises:\n",
    "    AssertionError: If the dataset or model is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    assert dataset in ('cifar10', 'cifar100', 'imagenet'), f'dataset {dataset} is currently not supported by this function'\n",
    "    assert model in ('mlp', 'cnn', 'vit'), f'model {model} is currently not supported by this function'\n",
    "\n",
    "    num_classes = CLASS_DICT[dataset]\n",
    "    eval_batch_size = 1024\n",
    "    feature_extractor = None\n",
    "\n",
    "    if dataset == 'imagenet':\n",
    "        data_resolution = 64\n",
    "        assert model == 'mlp', f'imagenet dataset is only supported by mlp model'\n",
    "    else:\n",
    "        data_resolution = 32\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device == 'cuda':\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    if model == 'mlp':\n",
    "        crop_resolution = 64\n",
    "        architecture = 'B_12-Wi_1024'\n",
    "        checkpoint = 'in21k_' + dataset\n",
    "\n",
    "        model = get_model(architecture=architecture, resolution=crop_resolution, num_classes=num_classes, checkpoint=checkpoint)\n",
    "\n",
    "    if model == 'cnn':\n",
    "        crop_resolution = 32\n",
    "        architecture = 'resnet18_' + dataset\n",
    "\n",
    "        model = timm.create_model(architecture, pretrained=True)\n",
    "\n",
    "    if model == 'vit':\n",
    "        crop_resolution = 32\n",
    "\n",
    "        if dataset == 'cifar10':\n",
    "            architecture = 'nateraw/vit-base-patch16-224-cifar10'\n",
    "            feature_extractor = ViTFeatureExtractor.from_pretrained(architecture)\n",
    "            model = ViTForImageClassification.from_pretrained(architecture)\n",
    "\n",
    "        elif dataset == 'cifar100':\n",
    "            architecture = 'Ahmed9275/Vit-Cifar100'\n",
    "            feature_extractor = AutoImageProcessor.from_pretrained(architecture)\n",
    "            model = AutoModelForImageClassification.from_pretrained(architecture)\n",
    "\n",
    "    data_loader = get_loader(\n",
    "        dataset,\n",
    "        bs=eval_batch_size,\n",
    "        mode=\"test\",\n",
    "        augment=False,\n",
    "        dev=device,\n",
    "        mixup=0.0,\n",
    "        data_path=data_path,\n",
    "        data_resolution=data_resolution,\n",
    "        crop_resolution=crop_resolution,\n",
    "    )\n",
    "\n",
    "    return data_loader, model, feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating baseline model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function that evaluates test accuracy\n",
    "@torch.no_grad()\n",
    "def test(model, loader, extractor = None):\n",
    "    model.eval()\n",
    "    total_acc, total_top5 = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        # ims = torch.reshape(ims, (ims.shape[0], -1))\n",
    "        if extractor is not None:\n",
    "            transformed_ims = [transforms.ToPILImage()(im) for im in ims]\n",
    "            inputs = extractor(images=transformed_ims, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits\n",
    "        else:\n",
    "            preds = model(ims)\n",
    "            \n",
    "        acc, top5 = topk_acc(preds, targs, k=5, avg=True)\n",
    "\n",
    "        total_acc.update(acc, ims.shape[0])\n",
    "        total_top5.update(top5, ims.shape[0])\n",
    "\n",
    "    return (\n",
    "        total_acc.get_avg(percentage=True),\n",
    "        total_top5.get_avg(percentage=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/ffcv/cifar10/val_32.beton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 10/10 [04:29<00:00, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy         94.4600\n",
      "Top 5 Test Accuracy           99.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader, model, feature_extractor = get_data_and_model(dataset='cifar10', model='cnn', data_path='/scratch/ffcv/')\n",
    "test_acc, test_top5 = test(model, data_loader, feature_extractor)\n",
    "\n",
    "# Print all the stats\n",
    "print(\"Test Accuracy        \", \"{:.4f}\".format(test_acc))\n",
    "print(\"Top 5 Test Accuracy          \", \"{:.4f}\".format(test_top5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate adversarial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(model, x_batch, target, k, eps, eps_step):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    x_adv = x_batch + eps * (2*torch.rand_like(x_batch) - 1)\n",
    "    x_adv.clamp_(min=0., max=1.)\n",
    "    \n",
    "    for _ in range(k):\n",
    "        x_adv.detach_().requires_grad_()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(x_adv)\n",
    "        loss_fn(out, target).backward()\n",
    "    \n",
    "        step = eps_step * x_adv.grad.sign()\n",
    "        x_adv = x_batch + (x_adv + step - x_batch).clamp_(min=-eps, max=eps)\n",
    "\n",
    "        x_adv.clamp_(min=0, max=1)\n",
    "\n",
    "    return x_adv.detach()\n",
    "\n",
    "def fgsm_untargeted(model, x, label, eps, clip_min=None, clip_max=None):\n",
    "    input_ = x.clone().detach_()\n",
    "    input_.requires_grad_()\n",
    "\n",
    "    logits = model(input_)\n",
    "    model.zero_grad()\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits, label)\n",
    "    loss.backward()\n",
    "    \n",
    "    out = input_ + eps * input_.grad.sign()\n",
    "    \n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        out.clamp_(min=clip_min, max=clip_max)\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adversarial(model, loader, eps, mode, epsStep = None):\n",
    "    model.eval()\n",
    "    total_acc, total_top5 = AverageMeter(), AverageMeter()\n",
    "    total_accFGSM, total_top5FGSM = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for ims, targs in tqdm(loader, desc=\"Evaluation\"):\n",
    "        ims = torch.reshape(ims, (ims.shape[0], -1))\n",
    "        targs = targs\n",
    "        if mode ==\"fgsm\":\n",
    "            imsFGSM = fgsm_untargeted(model, ims, targs, eps, clip_min=None, clip_max=None)\n",
    "        if mode == \"pgd\":\n",
    "            imsFGSM = pgd(model, ims, targs, 5, eps, epsStep)\n",
    "\n",
    "        preds = model(ims)\n",
    "        predsFGSM = model(imsFGSM)\n",
    "   \n",
    "        acc, top5 = topk_acc(preds, targs, k=5, avg=True)\n",
    "        accFGSM, top5FGSM = topk_acc(predsFGSM, targs, k=5, avg=True)\n",
    "\n",
    "        total_acc.update(acc, ims.shape[0])\n",
    "        total_top5.update(top5, ims.shape[0])\n",
    "\n",
    "        total_accFGSM.update(accFGSM, ims.shape[0])\n",
    "        total_top5FGSM.update(top5FGSM, ims.shape[0])\n",
    "\n",
    "    return (\n",
    "        total_acc.get_avg(percentage=True),\n",
    "        total_top5.get_avg(percentage=True),\n",
    "        total_accFGSM.get_avg(percentage=True),\n",
    "        total_top5FGSM.get_avg(percentage=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, model, feature_extractor = get_data_and_model(dataset='cifar10', model='mlp', data_path='/scratch/ffcv/')\n",
    "test_adversarial(model, data_loader, 0.05, 'fgsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, model, feature_extractor = get_data_and_model(dataset='cifar10', model='cnn', data_path='/scratch/ffcv/')\n",
    "test_adversarial(model, data_loader, 0.05, 'fgsm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
